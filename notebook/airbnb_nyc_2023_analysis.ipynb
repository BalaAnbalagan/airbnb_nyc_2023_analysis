{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3846509",
   "metadata": {},
   "source": [
    "# NYC Airbnb 2023 Data Science Analysis\n",
    "\n",
    "This notebook reproduces the data cleaning, exploratory analysis, modeling and clustering steps used in our New York City Airbnb pricing project. Upload the `listings.csv` file from the Inside Airbnb 2023 dataset to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sns.set(style='whitegrid', context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Upload the listings.csv file to Colab and set the correct path here\n",
    "# For example, after uploading you can use:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df_raw = pd.read_csv('listings.csv')\n",
    "\n",
    "file_path = 'listings.csv'  # update this if needed\n",
    "\n",
    "# Load the dataset\n",
    "print('Loading data...')\n",
    "df_raw = pd.read_csv(file_path)\n",
    "print(df_raw.head())\n",
    "print('Shape:', df_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cdbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert price to numeric by removing dollar signs and commas\n",
    "# Remove rows with missing price or latitude/longitude\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Clean price\n",
    "price_col = df['price'].astype(str).str.replace('[\\$,]', '', regex=True)\n",
    "df['price'] = pd.to_numeric(price_col, errors='coerce')\n",
    "\n",
    "# Drop rows with missing critical values\n",
    "cols_to_check = ['price', 'minimum_nights', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
    "df = df.dropna(subset=cols_to_check)\n",
    "\n",
    "# Parse last_review to datetime and compute days_since_last_review\n",
    "from datetime import datetime\n",
    "\n",
    "df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')\n",
    "df['days_since_last_review'] = (pd.Timestamp.now() - df['last_review']).dt.days\n",
    "\n",
    "# Create additional features\n",
    "# Cap price at $500 to remove extreme outliers\n",
    "cap_value = 500\n",
    "df['price_capped'] = df['price'].clip(upper=cap_value)\n",
    "\n",
    "# Log transform price and capped price (add 1 to avoid log(0))\n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "df['log_price_capped'] = np.log1p(df['price_capped'])\n",
    "\n",
    "# Minimum nights transformation\n",
    "cap_min_nights = 30\n",
    "df['minimum_nights_capped'] = df['minimum_nights'].clip(upper=cap_min_nights)\n",
    "df['log_minimum_nights'] = np.log1p(df['minimum_nights'])\n",
    "\n",
    "# Host listing count transformation\n",
    "df['log_host_listings_count'] = np.log1p(df['calculated_host_listings_count'])\n",
    "\n",
    "# Reviews transformations\n",
    "df['log_reviews_per_month'] = np.log1p(df['reviews_per_month'])\n",
    "\n",
    "# Days since last review transformation\n",
    "# Fill missing days_since_last_review with a large number\n",
    "max_days = df['days_since_last_review'].max()\n",
    "df['days_since_last_review'] = df['days_since_last_review'].fillna(max_days)\n",
    "df['log_days_since_last_review'] = np.log1p(df['days_since_last_review'])\n",
    "\n",
    "# Display processed data\n",
    "print(df.head())\n",
    "print('Processed shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of capped price and log price\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(df['price'], bins=50, kde=True, ax=axs[0])\n",
    "axs[0].set_title('Distribution of Price')\n",
    "axs[0].set_xlabel('Price (USD)')\n",
    "\n",
    "sns.histplot(df['log_price'], bins=50, kde=True, ax=axs[1])\n",
    "axs[1].set_title('Distribution of Log Price')\n",
    "axs[1].set_xlabel('Log Price')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap of numeric variables\n",
    "numeric_cols = ['price', 'minimum_nights', 'calculated_host_listings_count',\n",
    "                'availability_365', 'reviews_per_month', 'days_since_last_review']\n",
    "\n",
    "corr = df[numeric_cols].corr()\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose target and features\n",
    "features = df[['minimum_nights', 'minimum_nights_capped',\n",
    "               'calculated_host_listings_count', 'log_host_listings_count',\n",
    "               'availability_365', 'reviews_per_month', 'log_reviews_per_month',\n",
    "               'days_since_last_review', 'log_days_since_last_review']]\n",
    "\n",
    "target = df['log_price_capped']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Baseline (mean)\n",
    "y_pred_mean = np.repeat(y_train.mean(), len(y_test))\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_mean))\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_mean)\n",
    "print(f\"Baseline (mean) RMSE: {rmse_baseline:.4f}, MAE: {mae_baseline:.4f}\")\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "numeric_features = features.columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_features)\n",
    "])\n",
    "\n",
    "# Models\n",
    "def evaluate_model(model, name):\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name}: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")\n",
    "\n",
    "# Linear regression\n",
    "evaluate_model(LinearRegression(), 'Linear Regression')\n",
    "# Ridge\n",
    "evaluate_model(Ridge(alpha=1.0), 'Ridge Regression')\n",
    "# Lasso\n",
    "evaluate_model(Lasso(alpha=0.001), 'Lasso Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest and Gradient Boosting on log price\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "gb = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "\n",
    "for model, name in [(rf, 'Random Forest'), (gb, 'Gradient Boosting')]:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name}: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans clustering on selected features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cluster_features = df[['price_capped', 'minimum_nights', 'calculated_host_listings_count',\n",
    "                       'availability_365', 'reviews_per_month', 'days_since_last_review']]\n",
    "\n",
    "scaler_cluster = StandardScaler()\n",
    "cluster_scaled = scaler_cluster.fit_transform(cluster_features)\n",
    "\n",
    "# Fit KMeans\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "df['kmeans_cluster'] = kmeans.fit_predict(cluster_scaled)\n",
    "\n",
    "# Cluster summary\n",
    "cluster_summary = df.groupby('kmeans_cluster')['price_capped'].agg(['median','mean','count'])\n",
    "print(cluster_summary)\n",
    "\n",
    "# DBSCAN on lat/lon\n",
    "coords = df_raw[['latitude','longitude']].dropna()\n",
    "scaler_geo = StandardScaler()\n",
    "coords_scaled = scaler_geo.fit_transform(coords)\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=20)\n",
    "coords['geo_cluster'] = dbscan.fit_predict(coords_scaled)\n",
    "\n",
    "# View counts per cluster\n",
    "print(coords['geo_cluster'].value_counts())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
